Base:
    model_root: './checkpoints/'
    num_workers: 3
    verbose: 1
    early_stop_patience: 2
    pickle_feature_encoder: True
    save_best_only: True
    eval_steps: null
    debug_mode: False
    group_id: null
    use_features: null
    feature_specs: null
    feature_config: null

DNN_test:
    model: DNN
    dataset_id: tiny_parquet
    loss: 'binary_crossentropy'
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 1.e-8
    net_regularizer: 0
    batch_size: 128
    embedding_dim: 4
    hidden_units: [64, 32]
    hidden_activations: relu
    net_dropout: 0
    batch_norm: False
    epochs: 1
    shuffle: True
    seed: 2019
    monitor: 'AUC'
    monitor_mode: 'max'

DNN_default: # This is a config template
    model: DNN
    dataset_id: TBD
    loss: 'binary_crossentropy'
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0
    net_regularizer: 0
    batch_size: 10000
    embedding_dim: 40
    hidden_units: [500, 500, 500]
    hidden_activations: relu
    net_dropout: 0
    batch_norm: False
    epochs: 100
    shuffle: True
    seed: 2019
    monitor: {'AUC': 1, 'logloss': -1}
    monitor_mode: 'max'

DNN_frappe:
    model: DNN
    dataset_id: frappe_x1
    loss: 'binary_crossentropy'
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0
    net_regularizer: 0
    batch_size: 10000
    embedding_dim: 16
    hidden_units: [500, 500, 500]
    hidden_activations: relu
    net_dropout: 0
    batch_norm: False
    epochs: 100
    shuffle: True
    seed: 2019
    monitor: {'AUC': 1, 'logloss': -1}
    monitor_mode: 'max'
    # streaming: True              # Must be True to use ParquetBlockDataLoader
    user_aggregate: True         # Enable user aggregation
    resplit_data: True
    use_split: True
    split_ratios: [0.8, 0.1, 0.1]
    # resplit_shuffle: True
    # min_user_records: 100        # Filter out users with records < 100
    # train_ratio: 0.8             # 80% of each user's data for training
    # valid_ratio: 0.1             # 10% for validation (remaining 10% for test)

DNN_cat_frappe:
    model: DNN
    dataset_id: frappe_x1
    loss: 'binary_crossentropy'
    metrics: ['logloss', 'AUC']
    task: binary_classification
    optimizer: adam
    learning_rate: 1.e-3
    embedding_regularizer: 0
    net_regularizer: 0
    batch_size: 10000
    embedding_dim: 16
    hidden_units: [500, 500, 500]
    hidden_activations: relu
    net_dropout: 0
    batch_norm: False
    epochs: 100
    shuffle: True
    seed: 2019
    monitor: {'AUC': 1, 'logloss': -1}
    monitor_mode: 'max'
    # Transfer learning related configuration
    enable_transfer_learning: True   # Enable transfer learning mode
    user_col: user                    # User column name, null means auto-detect (will search for columns containing 'user')
    min_records: 15                    # Filter out users with too few records
    resplit_shuffle: True             # Whether to shuffle data when splitting